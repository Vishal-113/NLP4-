{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal-113/NLP4-/blob/main/PyTorch_Scaled_Dot_Product_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "BATCH_SIZE = 2\n",
        "SEQ_LEN = 5         # Sequence length (L)\n",
        "D_K = 8             # Dimension of Q, K, V (d_k)\n",
        "# Note: For simplicity in printing, we omit the NUM_HEADS dimension here,\n",
        "# treating D_K as the last dimension (d_k) and SEQ_LEN as the sequence length (L).\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"Testing Scaled Dot-Product Attention with PyTorch.\")\n",
        "print(f\"Inputs: B={BATCH_SIZE}, L={SEQ_LEN}, D_K={D_K}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# 1. Create Random Input Tensors\n",
        "# Q, K, V shape: (batch_size, seq_len, d_k)\n",
        "Q = torch.randn(BATCH_SIZE, SEQ_LEN, D_K, dtype=torch.float32)\n",
        "K = torch.randn(BATCH_SIZE, SEQ_LEN, D_K, dtype=torch.float32)\n",
        "V = torch.randn(BATCH_SIZE, SEQ_LEN, D_K, dtype=torch.float32)\n",
        "\n",
        "def scaled_dot_product_attention_pytorch(Q, K, V):\n",
        "    \"\"\"\n",
        "    Computes Scaled Dot-Product Attention: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V\n",
        "\n",
        "    Q, K, V shapes: (B, L, D_K)\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)\n",
        "\n",
        "    # 1. Calculate raw scores (Dot product: Q * K^T)\n",
        "    # K.transpose(-2, -1) changes (B, L, D_K) to (B, D_K, L)\n",
        "    raw_scores = torch.matmul(Q, K.transpose(-2, -1)) # Shape: (B, L, L)\n",
        "\n",
        "    # --- Softmax Stability Check 1: Before Scaling ---\n",
        "    # Print max raw score to show potential instability before scaling\n",
        "    max_raw_score = raw_scores.max().item()\n",
        "    print(f\"\\n[Softmax Stability Check 1]\")\n",
        "    print(f\"Max Raw Score (Q*K^T): {max_raw_score:.4f} (Can be large, leading to vanishing gradients)\")\n",
        "\n",
        "    # 2. Scale the scores\n",
        "    scale_factor = torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "    scaled_scores = raw_scores / scale_factor\n",
        "\n",
        "    # --- Softmax Stability Check 2: After Scaling ---\n",
        "    # Print max scaled score to show how division by sqrt(d_k) stabilizes the input\n",
        "    max_scaled_score = scaled_scores.max().item()\n",
        "    print(f\"Max Scaled Score:      {max_scaled_score:.4f} (Stabilized input for softmax)\")\n",
        "\n",
        "    # 3. Apply Softmax to get Attention Weights\n",
        "    # The softmax is applied along the last dimension (the key dimension)\n",
        "    attention_weights = F.softmax(scaled_scores, dim=-1) # Shape: (B, L, L)\n",
        "\n",
        "    # 4. Multiply with V to get the output vectors\n",
        "    output = torch.matmul(attention_weights, V) # Shape: (B, L, D_K)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Run the attention function\n",
        "output_vectors, attn_weights = scaled_dot_product_attention_pytorch(Q, K, V)\n",
        "\n",
        "\n",
        "# --- Print Results (Step 3) ---\n",
        "\n",
        "# 3a. Attention Weight Matrix\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"3a. Attention Weight Matrix (Batch 0)\")\n",
        "print(\"Weights shape: (L_query, L_key) = (5, 5). Rows sum to 1.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Print the attention matrix for the first element in the batch\n",
        "attn_matrix_b0 = attn_weights[0].detach().numpy()\n",
        "# Format and print header/rows\n",
        "header = \"{:<10}\".format(\"Query\\\\Key\") + \"\".join(f\"{j:<10}\" for j in range(SEQ_LEN))\n",
        "print(header)\n",
        "print(\"-\" * len(header))\n",
        "\n",
        "for i in range(SEQ_LEN):\n",
        "    row = f\"Q[{i}]:<4\" + \"\".join(f\"{attn_matrix_b0[i, j]:<10.4f}\" for j in range(SEQ_LEN))\n",
        "    print(f\"Q[{i}]:     \" + \"\".join(f\"{attn_matrix_b0[i, j]:<10.4f}\" for j in range(SEQ_LEN)))\n",
        "\n",
        "# 3b. Output Vectors\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"3b. Output Vectors (Batch 1)\")\n",
        "print(\"Output shape: (L, D_K) = (5, 8). These are the contextualized embeddings.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Print the output vectors for the second element in the batch\n",
        "output_b1 = output_vectors[1].detach().numpy()\n",
        "\n",
        "for i in range(SEQ_LEN):\n",
        "    # Print first 4 dimensions of the vector\n",
        "    vector_snippet = ', '.join(f'{x:.4f}' for x in output_b1[i, :4])\n",
        "    print(f\"Output for Pos {i}: [{vector_snippet} ...]\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "Testing Scaled Dot-Product Attention with PyTorch.\n",
            "Inputs: B=2, L=5, D_K=8\n",
            "------------------------------------------------------------\n",
            "\n",
            "[Softmax Stability Check 1]\n",
            "Max Raw Score (Q*K^T): 4.8066 (Can be large, leading to vanishing gradients)\n",
            "Max Scaled Score:      1.6994 (Stabilized input for softmax)\n",
            "\n",
            "============================================================\n",
            "3a. Attention Weight Matrix (Batch 0)\n",
            "Weights shape: (L_query, L_key) = (5, 5). Rows sum to 1.\n",
            "============================================================\n",
            "Query\\Key 0         1         2         3         4         \n",
            "------------------------------------------------------------\n",
            "Q[0]:     0.2815    0.1595    0.1294    0.3631    0.0665    \n",
            "Q[1]:     0.1579    0.1584    0.2353    0.1935    0.2549    \n",
            "Q[2]:     0.2155    0.5677    0.0260    0.1256    0.0651    \n",
            "Q[3]:     0.1026    0.3303    0.0869    0.3324    0.1478    \n",
            "Q[4]:     0.1031    0.1305    0.3847    0.0949    0.2868    \n",
            "\n",
            "============================================================\n",
            "3b. Output Vectors (Batch 1)\n",
            "Output shape: (L, D_K) = (5, 8). These are the contextualized embeddings.\n",
            "============================================================\n",
            "Output for Pos 0: [-0.3054, 0.0132, -1.2570, -0.2923 ...]\n",
            "Output for Pos 1: [-0.1123, 0.7079, -1.0126, -0.3932 ...]\n",
            "Output for Pos 2: [-0.1275, 0.6862, -1.0497, -0.3371 ...]\n",
            "Output for Pos 3: [-0.0435, 0.8794, -0.6292, -0.8287 ...]\n",
            "Output for Pos 4: [-0.0661, 0.7909, -0.8462, -0.6433 ...]\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x2yIbhEfWT3",
        "outputId": "1ded4ad3-e579-4f49-ae42-e9c06d676b26"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}