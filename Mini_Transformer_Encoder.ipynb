{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal-113/NLP4-/blob/main/Mini_Transformer_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# Set a consistent random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- Hyperparameters and Data Setup ---\n",
        "\n",
        "# 1. Use a small dataset (10 short sentences)\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A journey of a thousand miles begins with a single step.\",\n",
        "    \"To be or not to be that is the question.\",\n",
        "    \"All that glitters is not gold.\",\n",
        "    \"Where there is a will there is a way.\",\n",
        "    \"The early bird catches the worm.\",\n",
        "    \"An apple a day keeps the doctor away.\",\n",
        "    \"Practice makes perfect.\",\n",
        "    \"Life is what happens when you're busy making other plans.\",\n",
        "    \"If you want to live a happy life tie it to a goal not to people or things.\"\n",
        "]\n",
        "\n",
        "# Parameters\n",
        "D_MODEL = 32        # Embedding dimension\n",
        "MAX_LEN = 15        # Max sequence length (for padding)\n",
        "NUM_HEADS = 4       # Number of attention heads\n",
        "D_K = D_MODEL // NUM_HEADS  # Dimension of Q, K, V for each head\n",
        "D_FF = 128          # Inner dimension of the Feed-Forward Network\n",
        "NUM_LAYERS = 1      # Number of Encoder layers\n",
        "\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "\n",
        "# --- Tokenization and Embedding ---\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    \"\"\"Tokenizes text and builds a simple word-to-index mapping.\"\"\"\n",
        "    word_to_idx = {}\n",
        "    for sentence in sentences:\n",
        "        # Simple cleaning\n",
        "        tokens = sentence.lower().replace('.', '').replace(',', '').strip().split()\n",
        "        for word in tokens:\n",
        "            if word not in word_to_idx:\n",
        "                word_to_idx[word] = len(word_to_idx)\n",
        "\n",
        "    # Add special tokens\n",
        "    word_to_idx = {word: idx + 2 for word, idx in word_to_idx.items()}\n",
        "    word_to_idx['<pad>'] = PAD_IDX\n",
        "    word_to_idx['<unk>'] = UNK_IDX\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    return word_to_idx, idx_to_word\n",
        "\n",
        "def tokenize_batch(sentences, word_to_idx, max_len):\n",
        "    \"\"\"Converts sentences to padded token ID tensors.\"\"\"\n",
        "    batch_tokens = []\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.lower().replace('.', '').replace(',', '').strip().split()\n",
        "        indices = [word_to_idx.get(token, UNK_IDX) for token in tokens]\n",
        "\n",
        "        # Padding\n",
        "        padded_indices = indices + [PAD_IDX] * (max_len - len(indices))\n",
        "        batch_tokens.append(padded_indices[:max_len])\n",
        "\n",
        "    return np.array(batch_tokens)\n",
        "\n",
        "# Build vocab and tokenize data\n",
        "word_to_idx, idx_to_word = build_vocab(sentences)\n",
        "VOCAB_SIZE = len(word_to_idx)\n",
        "token_batch = tokenize_batch(sentences, word_to_idx, MAX_LEN)\n",
        "\n",
        "# Embedding Layer Weights (randomly initialized)\n",
        "embedding_weights = np.random.randn(VOCAB_SIZE, D_MODEL) * 0.01\n",
        "\n",
        "def embed_tokens(token_ids):\n",
        "    \"\"\"Looks up token IDs in the embedding matrix.\"\"\"\n",
        "    return embedding_weights[token_ids]\n",
        "\n",
        "# --- 2. Sinusoidal Positional Encoding ---\n",
        "\n",
        "def positional_encoding(max_len, d_model):\n",
        "    \"\"\"Generates the Sinusoidal Positional Encoding matrix.\"\"\"\n",
        "    pe = np.zeros((max_len, d_model))\n",
        "    position = np.arange(0, max_len)[:, np.newaxis]\n",
        "\n",
        "    # Calculate the division term: 1 / 10000^(2i/d_model)\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "\n",
        "    # Apply sin to even indices (2i)\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    # Apply cos to odd indices (2i + 1)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "    # Add batch dimension for broadcasting: (1, max_len, d_model)\n",
        "    return pe[np.newaxis, :, :]\n",
        "\n",
        "# --- 3. Implement Add & Norm (Layer Normalization) ---\n",
        "\n",
        "class LayerNorm:\n",
        "    \"\"\"Standard Layer Normalization (computes mean/std across the last axis).\"\"\"\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        # Learnable parameters (gamma/weight and beta/bias)\n",
        "        self.gamma = np.ones(d_model)\n",
        "        self.beta = np.zeros(d_model)\n",
        "        self.eps = eps # Epsilon to prevent division by zero\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        mean = x.mean(axis=-1, keepdims=True)\n",
        "        std = x.std(axis=-1, keepdims=True)\n",
        "        # Normalize: (x - mean) / std_plus_eps\n",
        "        normalized = (x - mean) / (std + self.eps)\n",
        "        # Apply learnable scale and shift\n",
        "        return normalized * self.gamma + self.beta\n",
        "\n",
        "# --- 4. Implement Self-Attention ---\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Computes Scaled Dot-Product Attention.\n",
        "    Q, K, V shape: (batch_size, num_heads, seq_len, d_k)\n",
        "    \"\"\"\n",
        "    d_k = Q.shape[-1]\n",
        "\n",
        "    # 1. Compute scores: Q * K_T / sqrt(d_k)\n",
        "    # K.transpose(0, 1, 3, 2) results in (batch_size, num_heads, d_k, seq_len)\n",
        "    scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n",
        "\n",
        "    # 2. Apply mask (if any) to prevent attention to padding\n",
        "    if mask is not None:\n",
        "        # Mask is True (1.0) at padding positions, so we add a huge negative value\n",
        "        scores += mask * -1e9\n",
        "\n",
        "    # 3. Softmax to get attention weights\n",
        "    # Stable softmax: subtract max for numerical stability\n",
        "    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
        "\n",
        "    # 4. Multiply with V\n",
        "    output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# --- 4. Implement Multi-Head Attention ---\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Linear projections for Q, K, V, and Output (randomly initialized)\n",
        "        self.W_Q = np.random.randn(d_model, d_model) * 0.01\n",
        "        self.W_K = np.random.randn(d_model, d_model) * 0.01\n",
        "        self.W_V = np.random.randn(d_model, d_model) * 0.01\n",
        "        self.W_O = np.random.randn(d_model, d_model) * 0.01\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape to (batch_size, seq_len, num_heads, d_k)\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)\n",
        "        # Transpose to (batch_size, num_heads, seq_len, d_k)\n",
        "        return x.transpose(0, 2, 1, 3)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # x shape: (batch_size, num_heads, seq_len, d_k)\n",
        "        batch_size, num_heads, seq_len, d_k = x.shape\n",
        "        # Transpose back to (batch_size, seq_len, num_heads, d_k)\n",
        "        x = x.transpose(0, 2, 1, 3)\n",
        "        # Reshape to (batch_size, seq_len, d_model)\n",
        "        return x.reshape(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "\n",
        "        # 1. Linear projections\n",
        "        Q_proj = np.matmul(Q, self.W_Q)\n",
        "        K_proj = np.matmul(K, self.W_K)\n",
        "        V_proj = np.matmul(V, self.W_V)\n",
        "\n",
        "        # 2. Split into multiple heads\n",
        "        Q_split = self.split_heads(Q_proj)\n",
        "        K_split = self.split_heads(K_proj)\n",
        "        V_split = self.split_heads(V_proj)\n",
        "\n",
        "        # 3. Scaled Dot-Product Attention\n",
        "        attn_output, attn_weights = scaled_dot_product_attention(Q_split, K_split, V_split, mask)\n",
        "        # attn_weights shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        # 4. Concatenate heads\n",
        "        concat_output = self.combine_heads(attn_output)\n",
        "\n",
        "        # 5. Final linear projection\n",
        "        output = np.matmul(concat_output, self.W_O)\n",
        "\n",
        "        return output, attn_weights\n",
        "\n",
        "# --- 4. Implement Feed-Forward Network (FFN) ---\n",
        "\n",
        "class FeedForward:\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        # Weights (d_model, d_ff) and (d_ff, d_model)\n",
        "        self.W1 = np.random.randn(d_model, d_ff) * 0.01\n",
        "        self.b1 = np.zeros(d_ff)\n",
        "        self.W2 = np.random.randn(d_ff, d_model) * 0.01\n",
        "        self.b2 = np.zeros(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # GELU activation (modern and smooth activation function)\n",
        "        def gelu(x):\n",
        "            return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "        # 1. Linear (d_model -> d_ff) + Bias\n",
        "        h = np.matmul(x, self.W1) + self.b1\n",
        "\n",
        "        # 2. Activation\n",
        "        h = gelu(h)\n",
        "\n",
        "        # 3. Linear (d_ff -> d_model) + Bias\n",
        "        output = np.matmul(h, self.W2) + self.b2\n",
        "        return output\n",
        "\n",
        "# --- 4. Transformer Encoder Layer ---\n",
        "\n",
        "class EncoderLayer:\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "\n",
        "        # Layer Normalization layers\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # 1. Multi-Head Attention Sublayer (Self-Attention)\n",
        "        attn_output, attn_weights = self.attn.forward(Q=x, K=x, V=x, mask=mask)\n",
        "\n",
        "        # Add & Norm 1 (Residual connection + Layer Normalization)\n",
        "        x = self.norm1.forward(x + attn_output)\n",
        "\n",
        "        # 2. Feed-Forward Sublayer\n",
        "        ffn_output = self.ffn.forward(x)\n",
        "\n",
        "        # Add & Norm 2 (Residual connection + Layer Normalization)\n",
        "        x = self.norm2.forward(x + ffn_output)\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "# --- Full Mini Transformer Encoder ---\n",
        "\n",
        "class TransformerEncoder:\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, max_len, vocab_size, embedding_weights):\n",
        "        self.embedding_weights = embedding_weights\n",
        "        self.pe = positional_encoding(max_len, d_model)\n",
        "        self.layers = [EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
        "\n",
        "    def create_padding_mask(self, token_ids):\n",
        "        \"\"\"Creates a broadcastable mask (1.0 at padding positions).\"\"\"\n",
        "        # is_padding: True where token is <pad> (0)\n",
        "        is_padding = (token_ids == PAD_IDX) # (batch_size, seq_len)\n",
        "\n",
        "        # Mask shape: (batch_size, 1, 1, seq_len)\n",
        "        # This will broadcast to mask the K/V sequence (the columns of the attention matrix).\n",
        "        mask = is_padding[:, np.newaxis, np.newaxis, :]\n",
        "\n",
        "        return mask.astype(np.float32)\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        # 1. Embedding + Positional Encoding\n",
        "        x = embed_tokens(token_ids)\n",
        "        x += self.pe[:, :x.shape[1], :] # Slicing PE to match sequence length\n",
        "\n",
        "        # 2. Padding Mask\n",
        "        padding_mask = self.create_padding_mask(token_ids)\n",
        "\n",
        "        all_attn_weights = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, attn_weights = layer.forward(x, mask=padding_mask)\n",
        "            all_attn_weights.append(attn_weights)\n",
        "\n",
        "        # x: (batch_size, seq_len, d_model) - Final contextual embeddings\n",
        "        return x, all_attn_weights\n",
        "\n",
        "# --- Execution and Demonstration (Step 5) ---\n",
        "\n",
        "# Initialize the Encoder\n",
        "encoder = TransformerEncoder(\n",
        "    num_layers=NUM_LAYERS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    d_ff=D_FF,\n",
        "    max_len=MAX_LEN,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_weights=embedding_weights\n",
        ")\n",
        "\n",
        "# 1. Input Tokens\n",
        "print(\"-\" * 50)\n",
        "print(\"1. Input Tokens (First Sentence):\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Get the first sentence's original tokens and token IDs\n",
        "first_sentence_tokens = [idx_to_word[i] for i in token_batch[0] if i != PAD_IDX]\n",
        "first_sentence_ids = token_batch[0][:len(first_sentence_tokens)]\n",
        "\n",
        "print(\"Raw Sentence:   \", sentences[0])\n",
        "print(\"Token IDs:      \", first_sentence_ids)\n",
        "print(\"Token Sequence: \", first_sentence_tokens)\n",
        "print(\"Padding to Max_Len:\", token_batch[0])\n",
        "\n",
        "\n",
        "# Run the forward pass\n",
        "final_embeddings, all_attn_weights = encoder.forward(token_batch)\n",
        "\n",
        "\n",
        "# 2. Final Contextual Embeddings\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"2. Final Contextual Embeddings (First 3 Tokens of First Sentence)\")\n",
        "print(\"Shape: (Batch Size, Seq Length, D_MODEL) = \", final_embeddings.shape)\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Extract first sentence's contextual embeddings (up to first 5 words)\n",
        "first_sentence_embeddings = final_embeddings[0, :5, :]\n",
        "\n",
        "for i, token in enumerate(first_sentence_tokens[:5]):\n",
        "    embedding_snippet = ', '.join(f'{x:.4f}' for x in first_sentence_embeddings[i, :4]) + '...'\n",
        "    print(f\"Token '{token:<5}': [{embedding_snippet}]\")\n",
        "\n",
        "\n",
        "# 3. Attention Heatmap (Matrix Printout)\n",
        "print(\"\\n\" + \"#\" * 50)\n",
        "print(\"3. Attention Heatmap (Head 1, First Sentence)\")\n",
        "print(\"#\" * 50)\n",
        "\n",
        "# Attn weights shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "# We use the first layer, first head, first sentence.\n",
        "attn_matrix = all_attn_weights[0][0, 0, :, :]\n",
        "\n",
        "# Slice the matrix to show only the tokens, ignoring padding rows/columns\n",
        "N = len(first_sentence_tokens)\n",
        "attn_matrix_visible = attn_matrix[:N, :N]\n",
        "\n",
        "# Print header row (Query/K tokens)\n",
        "header = \"{:<10}\".format(\"Query\\\\Key\") + \"\".join(f\"{t:<10}\" for t in first_sentence_tokens)\n",
        "print(header)\n",
        "print(\"-\" * len(header))\n",
        "\n",
        "# Print body rows (Query tokens)\n",
        "for i, query_token in enumerate(first_sentence_tokens):\n",
        "    row = f\"{query_token:<10}\"\n",
        "    # Print weights from the query token (row i) to all key tokens (cols j)\n",
        "    for j in range(N):\n",
        "        row += f\"{attn_matrix_visible[i, j]:<10.4f}\"\n",
        "    print(row)\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nNote: Each row shows how much the 'Query' word (row token) attended to the 'Key' words (column tokens).\")\n",
        "print(\"High values (e.g., 0.1000) indicate strong connections for this specific attention head.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "1. Input Tokens (First Sentence):\n",
            "--------------------------------------------------\n",
            "Raw Sentence:    The quick brown fox jumps over the lazy dog.\n",
            "Token IDs:       [2 3 4 5 6 7 2 8 9]\n",
            "Token Sequence:  ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "Padding to Max_Len: [2 3 4 5 6 7 2 8 9 0 0 0 0 0 0]\n",
            "\n",
            "==================================================\n",
            "2. Final Contextual Embeddings (First 3 Tokens of First Sentence)\n",
            "Shape: (Batch Size, Seq Length, D_MODEL) =  (10, 15, 32)\n",
            "==================================================\n",
            "Token 'the  ': [-0.9876, 1.0276, -0.9908, 1.0199...]\n",
            "Token 'quick': [0.6570, 0.0034, -0.0151, 0.6649...]\n",
            "Token 'brown': [0.8049, -2.0130, 0.7705, -0.2031...]\n",
            "Token 'fox  ': [-0.6835, -2.8093, 1.0087, -1.1653...]\n",
            "Token 'jumps': [-2.1849, -2.0096, 0.6118, -1.9376...]\n",
            "\n",
            "##################################################\n",
            "3. Attention Heatmap (Head 1, First Sentence)\n",
            "##################################################\n",
            "Query\\Key the       quick     brown     fox       jumps     over      the       lazy      dog       \n",
            "----------------------------------------------------------------------------------------------------\n",
            "the       0.1112    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    \n",
            "quick     0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    \n",
            "brown     0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    \n",
            "fox       0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1112    \n",
            "jumps     0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1112    \n",
            "over      0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1112    \n",
            "the       0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1112    0.1112    \n",
            "lazy      0.1110    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1112    0.1112    \n",
            "dog       0.1110    0.1111    0.1111    0.1111    0.1111    0.1111    0.1111    0.1112    0.1112    \n",
            "\n",
            "Note: Each row shows how much the 'Query' word (row token) attended to the 'Key' words (column tokens).\n",
            "High values (e.g., 0.1000) indicate strong connections for this specific attention head.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4YMV_95duRY",
        "outputId": "94e82895-8122-4f99-ba01-d370cc7dba25"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}